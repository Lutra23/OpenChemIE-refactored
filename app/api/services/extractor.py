"""
æå–å™¨æœåŠ¡æ¨¡å—
åŒ…å«æ¨¡åž‹ç®¡ç†ã€å¼‚æ­¥ä»»åŠ¡å¤„ç†ç­‰æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
"""

import os
import gc
import json
import uuid
import threading
import time
import queue
import torch
from datetime import datetime
from pathlib import Path
from functools import lru_cache
from werkzeug.utils import secure_filename
import concurrent.futures
import shutil
from fastapi import HTTPException

# å¯¼å…¥æˆ‘ä»¬çš„æå–å™¨
from ...openchemie.extractor import OpenChemIEExtractorV2

# å…¨å±€å˜é‡
model_manager = None
executor = None
task_status = {}
task_status_lock = threading.Lock() # å¼•å…¥ä¸€ä¸ªé”

class ModelManager:
    """æ¨¡åž‹ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼"""
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.models = {}
            self._model_lock = threading.Lock()
            self._initialized = True
    
    def get_model(self, device='auto'):
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        with self._model_lock:
            if device not in self.models:
                print(f"ðŸ“± åˆå§‹åŒ–æ¨¡åž‹ (è®¾å¤‡: {device})...")
                self.models[device] = OpenChemIEExtractorV2(device=device)
                if device == 'cuda' and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            return self.models[device]
    
    def cleanup_models(self):
        with self._model_lock:
            for device, model in self.models.items():
                del model
                if device == 'cuda' and torch.cuda.is_available():
                    torch.cuda.empty_cache()
            self.models.clear()
            gc.collect()

def cleanup_memory():
    """æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def create_result_summary(results):
    """åˆ›å»ºç»“æžœæ‘˜è¦"""
    try:
        stats = results.get('statistics', {})
        return {
            'summary': stats.get('document_overview', {}),
            'quality': results.get('quality_metrics', {}).get('overall_score', 0),
            'processing_time': results.get('metadata', {}).get('extraction_info', {}).get('processing_time_seconds', 0),
            'entities_count': {
                'molecules': len(results.get('chemical_entities', {}).get('molecules', {})),
                'reactions': len(results.get('chemical_entities', {}).get('reactions', {})),
                'figures': len(results.get('document_content', {}).get('figures', {})),
                'tables': len(results.get('document_content', {}).get('tables', {}))
            }
        }
    except:
        return {}

def update_batch_status(batch_id, task_status_dict):
    """æ›´æ–°æ‰¹å¤„ç†ä»»åŠ¡çš„æ€»ä½“çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with task_status_lock:
        if batch_id in task_status_dict and task_status_dict[batch_id]['type'] == 'batch':
            batch_task = task_status_dict[batch_id]
            
            completed_count = 0
            failed_count = 0
            
            for task_id, subtask_info in batch_task['subtasks'].items():
                # ä»Žä¸»çŠ¶æ€å­—å…¸èŽ·å–æœ€æ–°çš„å­ä»»åŠ¡çŠ¶æ€
                subtask_status = task_status_dict.get(task_id, {}).get('status')
                if subtask_status:
                    batch_task['subtasks'][task_id]['status'] = subtask_status

                if subtask_status == 'completed':
                    completed_count += 1
                elif subtask_status == 'error' or subtask_status == 'failed': # å‡è®¾ 'error' å’Œ 'failed' éƒ½æ˜¯å¤±è´¥çŠ¶æ€
                    failed_count += 1
            
            batch_task['completed_files'] = completed_count
            batch_task['failed_files'] = failed_count
            
            if (completed_count + failed_count) == batch_task['total_files']:
                batch_task['status'] = 'completed'
                batch_task['completion_time'] = datetime.now().isoformat()

def extract_task_optimized(task_id, pdf_path, options, task_status_dict, results_folder):
    """ä¼˜åŒ–çš„æå–ä»»åŠ¡å¤„ç†"""
    batch_id = None
    try:
        with task_status_lock:
            task_status_dict[task_id]['status'] = 'processing'
            task_status_dict[task_id]['message'] = 'æ­£åœ¨èŽ·å–æ¨¡åž‹...'
            batch_id = task_status_dict[task_id].get('batch_id')

        device = options.get('device', 'auto')
        extractor = model_manager.get_model(device)
        
        task_status_dict[task_id]['message'] = 'æ­£åœ¨æå–åŒ–å­¦ä¿¡æ¯...'
        
        results = extractor.extract_from_pdf(
            pdf_path,
            output_images=options.get('output_images', False),
            output_bbox=options.get('output_bbox', True),
            extract_corefs=options.get('extract_corefs', False)
        )
        
        result_file = os.path.join(results_folder, f"{task_id}_results.json")
        # ä¿å­˜ç»“æžœåˆ°æ–‡ä»¶
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        summary_results = create_result_summary(results)
        
        task_status_dict[task_id].update({
            'status': 'completed',
            'message': 'æå–å®Œæˆ',
            'result_file': result_file,
            'results': summary_results,
            'completion_time': datetime.now().isoformat()
        })
        
        del results
        cleanup_memory()
        
    except Exception as e:
        with task_status_lock:
            task_status_dict[task_id]['status'] = 'failed' # ä½¿ç”¨ 'failed' çŠ¶æ€
            task_status_dict[task_id]['error'] = f'æå–å¤±è´¥: {str(e)}' # æ·»åŠ  error å­—æ®µ
            batch_id = task_status_dict[task_id].get('batch_id')
        cleanup_memory()
    finally:
        # ç¡®ä¿æ€»ä¼šæ›´æ–°æ‰¹å¤„ç†çŠ¶æ€
        if batch_id:
            update_batch_status(batch_id, task_status_dict)
            
        # æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
        try:
            os.remove(pdf_path)
        except:
            pass

def task_worker(task_queue, task_status_dict, results_folder):
    """ä»»åŠ¡å·¥ä½œçº¿ç¨‹"""
    while True:
        try:
            task_data = task_queue.get(timeout=1)
            if task_data is None:
                break
            task_id, pdf_path, options = task_data
            extract_task_optimized(task_id, pdf_path, options, task_status_dict, results_folder)
            task_queue.task_done()
        except queue.Empty:
            continue

def initialize_queue_and_workers(settings, task_status_dict):
    """åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—å’Œå·¥ä½œçº¿ç¨‹æ± """
    global model_manager, executor
    
    # åˆ›å»ºæ¨¡åž‹ç®¡ç†å™¨
    model_manager = ModelManager()

    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    task_queue = queue.Queue(maxsize=settings.task_queue_size)

    # åˆ›å»ºçº¿ç¨‹æ± 
    executor = concurrent.futures.ThreadPoolExecutor(
        max_workers=settings.max_concurrent_tasks,
        thread_name_prefix='Extractor'
    )

    # å¯åŠ¨å·¥ä½œçº¿ç¨‹
    for i in range(settings.max_concurrent_tasks):
        executor.submit(worker, task_queue, model_manager, task_status_dict, settings.results_folder)
    
    print(f"âœ… {settings.max_concurrent_tasks} ä¸ªå·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
    return task_queue # è¿”å›žé˜Ÿåˆ—å®žä¾‹

def submit_task(pdf_file, options, task_status_dict, upload_folder, results_folder, task_queue, batch_id=None):
    """
    å°†æå–ä»»åŠ¡æäº¤åˆ°é˜Ÿåˆ—
    
    è¿”å›ž:
        str: ä»»åŠ¡ID
    """
    if task_queue is None:
        raise ValueError("ä»»åŠ¡é˜Ÿåˆ—æœªåˆå§‹åŒ–")

    task_id = str(uuid.uuid4())
    filename = f"{task_id}_{secure_filename(pdf_file.filename)}"
    pdf_path = os.path.join(upload_folder, filename)
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    with open(pdf_path, "wb") as buffer:
        shutil.copyfileobj(pdf_file.file, buffer)
    
    # å‡†å¤‡ä»»åŠ¡è¯¦æƒ…
    task_details = {
        'id': task_id,
        'type': 'single', # æ·»åŠ ç±»åž‹å­—æ®µ
        'batch_id': batch_id, # æ·»åŠ  batch_id
        'filename': pdf_file.filename,
        'saved_path': pdf_path,
        'options': options,
        'status': 'queued',
        'submitted_at': datetime.now().isoformat(),
        'result': None,
        'error': None
    }
    
    # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
    try:
        task_queue.put_nowait((task_id, pdf_path, options))
        with task_status_lock:
            task_status_dict[task_id] = task_details
        print(f"âœ… ä»»åŠ¡ {task_id} å·²æäº¤åˆ°é˜Ÿåˆ— (æ‰¹å¤„ç†: {batch_id or 'æ— '})")
        return task_id
    except queue.Full:
        raise HTTPException(status_code=503, detail="æœåŠ¡æ­£å¿™ï¼Œä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åŽå†è¯•")

def worker(task_queue, model_manager, task_status_dict, results_folder):
    """ä»»åŠ¡å·¥ä½œçº¿ç¨‹"""
    while True:
        try:
            task_data = task_queue.get(timeout=1)
            if task_data is None:
                break
            task_id, pdf_path, options = task_data
            extract_task_optimized(task_id, pdf_path, options, task_status_dict, results_folder)
            task_queue.task_done()
        except queue.Empty:
            continue

def cleanup_old_tasks(task_status_dict, max_age_hours=24, force=False):
    """æ¸…ç†æ—§ä»»åŠ¡"""
    cleanup_count = 0
    now = datetime.now()
    for task_id, data in list(task_status_dict.items()):
        upload_time_str = data.get('upload_time')
        if upload_time_str:
            upload_time = datetime.fromisoformat(upload_time_str)
            age = now - upload_time
            if age.total_seconds() > max_age_hours * 3600 or force:
                result_file = data.get('result_file')
                if result_file and os.path.exists(result_file):
                    os.remove(result_file)
                del task_status_dict[task_id]
                cleanup_count += 1
    return cleanup_count 